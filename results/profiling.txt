Number of Samples: 5000
Number of Components: 2048
Number of Features: 400
Number of Nonzero Coefficients: 17


Single core. New implementation of algorithm v0.
Samples per second: 505.5775722229696


Single core. Implementation of algorithm v0.
Samples per second: 409.0881585308904


8.881784197001252e-16
Single core. Naive Implementation, based on our Homework.
Samples per second: 133.1333524819922


Wrote profile results to test_omp.py.lprof
Timer unit: 1e-07 s

Total time: 10.2627 s
File: test_omp.py
Function: omp_v0 at line 63

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    63                                           @profile
    64                                           def omp_v0(y, X, XTX, XTy, n_nonzero_coefs=None):
    65         1         48.0     48.0      0.0      if n_nonzero_coefs is None:
    66                                                   n_nonzero_coefs = X.shape[1]
    67
    68         1         44.0     44.0      0.0      N = y.shape[1]
    69         1         28.0     28.0      0.0      innerp = lambda x: np.einsum('ij,ij->i', x, x)
    70         1      26273.0  26273.0      0.0      normr2 = innerp(y.T)  # Norm squared of residual.
    71         1         24.0     24.0      0.0      projections = XTy
    72
    73         1        641.0    641.0      0.0      gamma = np.zeros(shape=(n_nonzero_coefs, N), dtype=np.int64)
    74         1      30308.0  30308.0      0.0      F = np.repeat(np.identity(n_nonzero_coefs, dtype=X.dtype)[np.newaxis], N, 0)
    75         1        793.0    793.0      0.0      a_F = np.zeros_like(X, shape=(n_nonzero_coefs, N, 1))
    76         1    3340502.0 3340502.0      3.3      D_mybest = np.zeros_like(X, shape=(N, XTX.shape[0], n_nonzero_coefs))
    77         1         20.0     20.0      0.0      temp_F_k_k = 1
    78         1        521.0    521.0      0.0      xests = np.zeros((N, X.shape[1]))
    79        18        353.0     19.6      0.0      for k in range(n_nonzero_coefs):
    80        17    8345562.0 490915.4      8.1          maxindices = np.argmax(projections * projections, 1)  # Maybe replace square with abs?
    81        17       3351.0    197.1      0.0          gamma[k] = maxindices
    82        17        259.0     15.2      0.0          if k == 0:
    83         1     248251.0 248251.0      0.2              new_D_mybest = XTX[maxindices, :]
    84                                                   else:
    85        16     243566.0  15222.9      0.2              D_mybest_maxindices = np.take_along_axis(D_mybest[:, :, :k], maxindices[:, None, None], 1).squeeze(1)
    86        16      23237.0   1452.3      0.0              temp_F_k_k = np.sqrt(1/(1-innerp(D_mybest_maxindices)))[:, None]
    87        16     505059.0  31566.2      0.5              F[:, :, k] = -temp_F_k_k * (F[:, :, :k] @ D_mybest_maxindices[:, :, None]).squeeze(-1)
    88        16      11952.0    747.0      0.0              F[:, k, k] = temp_F_k_k[..., 0]
    89        16   42432658.0 2652041.1     41.3              new_D_mybest = temp_F_k_k * (XTX[maxindices, :] - (D_mybest[:, :, :k] @ D_mybest_maxindices[:, :, None]).squee
ze(-1))
    90        17   19934704.0 1172629.6     19.4          D_mybest[:, :, k] = new_D_mybest
    91        17      57720.0   3395.3      0.1          a_F[k] = temp_F_k_k * np.take_along_axis(projections, maxindices[:, None], 1)  # TODO: Figure out if we should use
XTy.T or XTy, which is faster?
    92        17   26873390.0 1580787.6     26.2          projections = projections - D_mybest[:, :, k] * a_F[k]
    93        17       8661.0    509.5      0.0          normr2 = normr2 - (a_F[k] * a_F[k]).squeeze(-1)
    94                                               else:
    95         1     538817.0 538817.0      0.5          np.put_along_axis(xests, gamma.T, (F @ a_F.squeeze(-1).T[:, :, None]).squeeze(-1), -1)
    96                                                   # Instead of putting a stopping criteria, we could return the order in which the coeffs were added. (i.e. just retu
rn gamma and amplitudes)
    97                                               # F[:, :k, :k] @ a_F.T[:k, :, None]
    98         1         18.0     18.0      0.0      return xests

Total time: 7.3563 s
File: test_omp.py
Function: omp_v0_new at line 129

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   129                                           @profile
   130                                           def omp_v0_new(y, X, XTX, XTy, n_nonzero_coefs=None):
   131         1         47.0     47.0      0.0      if n_nonzero_coefs is None:
   132                                                   n_nonzero_coefs = X.shape[1]
   133
   134         1         55.0     55.0      0.0      N = y.shape[1]
   135         1         35.0     35.0      0.0      innerp = lambda x: np.einsum('ij,ij->i', x, x)
   136         1      23496.0  23496.0      0.0      normr2 = innerp(y.T)  # Norm squared of residual.
   137         1         19.0     19.0      0.0      projections = XTy
   138
   139         1       4252.0   4252.0      0.0      gamma = np.zeros(shape=(n_nonzero_coefs, N), dtype=np.int64)
   140         1      38353.0  38353.0      0.1      F = np.repeat(np.identity(n_nonzero_coefs, dtype=X.dtype)[np.newaxis], N, 0)
   141         1       7551.0   7551.0      0.0      a_F = np.zeros_like(X, shape=(n_nonzero_coefs, N, 1))
   142         1    4898438.0 4898438.0      6.7      D_mybest = np.zeros_like(X, shape=(n_nonzero_coefs, N, XTX.shape[0]))
   143         1         58.0     58.0      0.0      temp_F_k_k = 1
   144         1        457.0    457.0      0.0      xests = np.zeros((N, X.shape[1]))
   145        18       2423.0    134.6      0.0      for k in range(n_nonzero_coefs):
   146        17    3774183.0 222010.8      5.1          maxindices = get_max_projections_blas(projections)  # Numba version is about twice as fast as: np.argmax(projection
s * projections, 1)  # Maybe replace square with abs?
   147                                                   # maxindices2 = np.argmax(projections * projections, 1)
   148        17       2741.0    161.2      0.0          gamma[k] = maxindices
   149        17        448.0     26.4      0.0          if k == 0:
   150         1     631365.0 631365.0      0.9              D_mybest[k] = XTX[None, maxindices, :]
   151                                                   else:
   152                                                       # Do something about this:
   153        16     394095.0  24630.9      0.5              D_mybest_maxindices = np.take_along_axis(D_mybest[:k, :, :], maxindices[None, :, None], 2).squeeze(2)
   154        16      27093.0   1693.3      0.0              temp_F_k_k = np.sqrt(1/(1-innerp(D_mybest_maxindices.T)))[:, None]
   155        16     671169.0  41948.1      0.9              F[:, :, k] = -temp_F_k_k * (F[:, :, :k] @ D_mybest_maxindices.T[:, :, None]).squeeze(-1)
   156        16      14373.0    898.3      0.0              F[:, k, k] = temp_F_k_k[..., 0]
   157                                                       # Could maybe be done with homogeneous coordinates, and einsum, but not sure if faster.
   158                                                       # testC = temp_F_k_k * XTX[None, maxindices, :] - np.einsum('ibj,ib,b->bj', D_mybest[:k], D_mybest_maxindices,
temp_F_k_k[..., 0], optimize='optimal')
   159        16   51863983.0 3241498.9     70.5              D_mybest[k] = temp_F_k_k * (XTX[None, maxindices, :] - np.einsum('ibj,ib->bj', D_mybest[:k], D_mybest_maxindices))
   160        17      92895.0   5464.4      0.1          a_F[k] = temp_F_k_k * np.take_along_axis(projections, maxindices[:, None], 1)
   161        17   10545013.0 620294.9     14.3          update_projections_blas(projections, D_mybest[k], -a_F[k, :, 0])  # Relativeely slow as all the subsets are different...
   162                                                   # projections2 = projections + (-a_F[k]) * D_mybest[k]  # Relativeely slow as all the subsets are different...
   163        17      10197.0    599.8      0.0          normr2 = normr2 - (a_F[k] * a_F[k]).squeeze(-1)
   164                                               else:
   165         1     560192.0 560192.0      0.8          np.put_along_axis(xests, gamma.T, (F @ a_F.squeeze(-1).T[:, :, None]).squeeze(-1), -1)
   166                                                   # Instead of putting a stopping criteria, we could return the order in which th   166                                                   # Instead of putting a stopping criteria, we could return the order in which the coeffs were added. (i.e. just return gamma and amplitudes)
   167                                               # F[:, :k, :k] @ a_F.T[:k, :, None]
   168         1         50.0     50.0      0.0      return xests


Newer results:
Settings used for the test:
Number of Samples: 50
Number of Components: 2048
Number of Features: 400
Number of Nonzero Coefficients: 128


Single core. Naive implementation.
Samples per second: 40.994570924994136


Single core. Implementation of algorithm v0.
Samples per second: 33.098672571116815


Single core. New implementation of algorithm v0.
Samples per second: 30.83907320443505


Single core. Sklearn
Samples per second: 9.287280705013078


1.021405182655144e-14
Wrote profile results to test_omp.py.lprof
Timer unit: 1e-07 s

Total time: 1.50289 s
File: test_omp.py
Function: omp_v0_new at line 182

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   182                                           @profile
   183                                           def omp_v0_new(y, X, XTX, XTy, n_nonzero_coefs=None):
   184                                               # TODO: Seems to be as fast as we are going to get, without custom kernels. We just need single-block memory alloc. (and send out-argument with pre-allocated mem to functions)
   185         1         33.0     33.0      0.0      if n_nonzero_coefs is None:
   186                                                   n_nonzero_coefs = X.shape[1]
   187
   188         1         27.0     27.0      0.0      B = y.shape[1]
   189         1         17.0     17.0      0.0      innerp = lambda x: np.einsum('ij,ij->i', x, x)
   190         1        778.0    778.0      0.0      normr2 = innerp(y.T)  # Norm squared of residual.
   191         1         11.0     11.0      0.0      projections = XTy
   192
   193         1        560.0    560.0      0.0      gamma = np.zeros(shape=(n_nonzero_coefs, B), dtype=np.int64)
   194         1      26332.0  26332.0      0.2      F = np.repeat(np.identity(n_nonzero_coefs, dtype=X.dtype)[np.newaxis], B, 0)
   195         1       1602.0   1602.0      0.0      a_F = np.zeros_like(X, shape=(n_nonzero_coefs, B, 1))
   196         1        561.0    561.0      0.0      D_mybest = np.empty_like(X, shape=(n_nonzero_coefs, B, XTX.shape[0]))  # empty_like is faster to init
   197         1         21.0     21.0      0.0      temp_F_k_k = 1
   198         1       8839.0   8839.0      0.1      xests = np.zeros((B, X.shape[1]))
   199       129       2843.0     22.0      0.0      for k in range(n_nonzero_coefs):
   200       128     161956.0   1265.3      1.1          maxindices = get_max_projections_blas(projections)  # Numba version is about twice as fast as: np.argmax(projections * projections, 1)  # Maybe replace square with abs?
   201       128       9745.0     76.1      0.1          gamma[k] = maxindices
   202       128       2545.0     19.9      0.0          if k == 0:
   203         1      15469.0  15469.0      0.1              D_mybest[k] = XTX[None, maxindices, :]
   204                                                   else:
   205                                                       # Do something about this:
   206       127     421405.0   3318.1      2.8              D_mybest_maxindices = np.take_along_axis(D_mybest[:k, :, :].transpose([2, 1, 0]), maxindices[None, :, None], 0).squeeze(0)
   207       127     117320.0    923.8      0.8              temp_F_k_k = np.sqrt(1/(1-innerp(D_mybest_maxindices)))[:, None]
   208       127     962178.0   7576.2      6.4              F[:, :, k] = -temp_F_k_k * (F[:, :, :k] @ D_mybest_maxindices[:, :, None]).squeeze(-1)
   209       127      11571.0     91.1      0.1              F[:, k, k] = temp_F_k_k[..., 0]
   210                                                       # Number of flops below: (2*k - 1) * D_mybest.shape[1] * D_mybest.shape[2]
   211                                                       # t1 = time.time()
   212       127    5012388.0  39467.6     33.4              D_mybest[k] = (D_mybest[:k].transpose([1, 2, 0]) @ D_mybest_maxindices[:, :, None]).squeeze(-1)  # <- faster than np.einsum('ibj,ib->bj', D_mybest[:k], D_mybest_maxindices)
   213                                                       # t2 = time.time()
   214                                                       # print(((2*k - 1) * D_mybest.shape[1] * D_mybest.shape[2] * 1e-9)/(t2-t1), 'GFLOPS')
   215       127    7881644.0  62060.2     52.4              update_D_mybest(temp_F_k_k, XTX, maxindices, D_mybest[k])
   216       128     175519.0   1371.2      1.2          a_F[k] = temp_F_k_k * np.take_along_axis(projections, maxindices[:, None], 1)
   217       128     189824.0   1483.0      1.3          update_projections_blast(projections, D_mybest[k], -a_F[k, :, 0])  # Around a 3x speedup :D
   218                                                   # projections2 = projections + (-a_F[k]) * D_mybest[k]  # Relativeely slow as all the subsets are different...
   219       128      18659.0    145.8      0.1          normr2 = normr2 - (a_F[k] * a_F[k]).squeeze(-1)
   220                                               else:
   221         1       7075.0   7075.0      0.0          np.put_along_axis(xests, gamma.T, (F @ a_F.squeeze(-1).T[:, :, None]).squeeze(-1), -1)
   222                                                   # Instead of putting a stopping criteria, we could return the order in which the coeffs were added. (i.e. just return gamma and amplitudes)
   223         1         14.0     14.0      0.0      return xests

Total time: 1.20708 s
File: test_omp.py
Function: omp_naive at line 259

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   259                                           @profile
   260                                           def omp_naive(X, y, n_nonzero_coefs):
   261         1      53902.0  53902.0      0.4      Xt = np.ascontiguousarray(X.T)
   262         1        802.0    802.0      0.0      y = np.ascontiguousarray(y.T)
   263         1        201.0    201.0      0.0      r = y.copy()  # Maybe no transpose? Remove this line?
   264         1        188.0    188.0      0.0      sets = np.zeros((n_nonzero_coefs, r.shape[0]), dtype=np.int32)
   265         1        338.0    338.0      0.0      problems = np.zeros((r.shape[0], n_nonzero_coefs, X.shape[0]))
   266         1      26491.0  26491.0      0.2      As = np.repeat(np.identity(n_nonzero_coefs, dtype=X.dtype)[np.newaxis], r.shape[0], 0)
   267                                               # solutions = np.zeros((r.shape[0], n_nonzero_coefs))
   268         1       7460.0   7460.0      0.1      xests = np.zeros_like(y, shape=(r.shape[0], X.shape[1]))
   269       129       4952.0     38.4      0.0      for k in range(n_nonzero_coefs):
   270                                                   #t1 = time.time()
   271       128    3153597.0  24637.5     26.1          projections = faster_projections(Xt, r).squeeze(-1)  # X.shape[0] * (2*X.shape[1]-1) * r.shape[0] = O(bNM), where X is an MxN matrix, N>M.
   272                                                   #t2 = time.time()
   273                                                   #print((X.shape[0] * (2*X.shape[1]-1) * r.shape[0] * 1e-9)/(t2-t1), 'GFLOPS')
   274       128     314955.0   2460.6      2.6          best_idxs = get_max_projections_blas(projections)  # best_idxs = np.abs(projections).squeeze(-1).argmax(1)  # O(bN), https://scikit-cuda.readthedocs.io/en/latest/generated/skcuda.misc.maxabs.html
   275       128      18769.0    146.6      0.2          sets[k, :] = best_idxs
   276       128      69728.0    544.8      0.6          best = Xt[best_idxs, :]  # A mess...
   277       128     177510.0   1386.8      1.5          problems[:, k, :] = best
   278       128       6434.0     50.3      0.1          current_problemst = problems[:, :k+1, :]
   279       128       6901.0     53.9      0.1          current_problems = current_problemst.transpose([0, 2, 1])
   280                                                   # TODO: We have already computed the result of current_problemst @ y[:, :, None]. (It is in projections I believe)
   281                                                   #       And similarly for the hermitian - it can be constructed from XTX.
   282                                                   # LAPACK has dgesvx/dgelsy (seems gelsy is newer/better) - gesv is for solving, these other ones work for least squares!
   283                                                   #  I think linalg.solve calls gesv, which uses LU factorization. https://stackoverflow.com/questions/55367024/fastest-way-of-solving-linear-least-squares
   284       128    1108083.0   8656.9      9.2          update = (current_problemst[:, :k, :] @ best[..., None]).squeeze(-1)
   285                                                   # Our algorithm could be so much faster if the lhs were not (likely) all different.
   286                                                   #  like in batch_mm, we could then treat the rhs as a single matrix.
   287       128      23611.0    184.5      0.2          As[:, k, :k] = update  # We only have to update the lower triangle for sposv to work!
   288                                                   if True:
   289       128      66003.0    515.6      0.5              As[:, :k, k] = update
   290       128       7315.0     57.1      0.1              solutions = np.linalg.solve(
   291       128       5053.0     39.5      0.0                   As[:, :k+1, :k+1],  # As[:, :k+1, :k+1],
   292       128    5861315.0  45791.5     48.6                   current_problemst @ y[:, :, None]).squeeze(-1)  # O(bk^2) memory.
   293                                                   else:
   294                                                       # ^ This is faster for small matrices (Python overhead most likely, but may also just be complexity)
   295                                                       solutions = solve_lstsq(current_problems, As[:, :k+1, :k+1], y)
   296       128    1154958.0   9023.1      9.6          r = y - (current_problems @ solutions[:, :, None]).squeeze(-1)
   297
   298                                                   # maybe memoize in case y is large, such that probability of repeats is significant.
   299                                                   # If we can move all the batched stuff to RHS (using dense representations) it may be faster.
   300                                                   # maybe memoize in case y is large, such that probability of repeats is significant. <- In case sparsity is low (e.g. < 32), this may be the fastest method, only limited by memory. (binomial(n, n*0.2
))   else:
   301                                                   # https://en.wikipedia.org/wiki/Singular_value_decomposition#Relation_to_eigenvalue_decomposition
   302                                                   # https://en.wikipedia.org/wiki/Woodbury_matrix_identity
   303                                                   # Test if _^2 is faster than abs(_)
   304                                               else:
   305                                                   # np.put_along_axis(xests.T, sets.T, solutions, 1)
   306         1       2187.0   2187.0      0.0          np.put_along_axis(xests, sets.T, solutions, 1)
   307
   308         1         11.0     11.0      0.0      return xests


(DrugDiscovery) C:\Users\sebse\OneDrive - Danmarks Tekniske Universitet\Kandidat\4sem\CompressedSensing\Project\omp-parallel-gpu-python>kernprof -l -v test_omp.py
Settings used for the test:
Number of Samples: 500
Number of Components: 2048
Number of Features: 400
Number of Nonzero Coefficients: 128


Single core. Naive implementation.
Samples per second: 40.02209027277024


Single core. Implementation of algorithm v0.
Samples per second: 26.803174447903388


Single core. New implementation of algorithm v0.
Samples per second: 57.11057563473924


Single core. Sklearn
Samples per second: 7.40746032410916


1.1546319456101628e-14
Wrote profile results to test_omp.py.lprof
Timer unit: 1e-07 s

Total time: 8.31793 s
File: test_omp.py
Function: omp_v0_new at line 182

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   182                                           @profile
   183                                           def omp_v0_new(y, X, XTX, XTy, n_nonzero_coefs=None):
   184                                               # TODO: Seems to be as fast as we are going to get, without custom kernels. We just need single-block memory alloc. (and send out-argument with pre-allocated mem to functions)
   185         1         65.0     65.0      0.0      if n_nonzero_coefs is None:
   186                                                   n_nonzero_coefs = X.shape[1]
   187
   188         1         59.0     59.0      0.0      B = y.shape[1]
   189         1         29.0     29.0      0.0      innerp = lambda x: np.einsum('ij,ij->i', x, x)
   190         1       7680.0   7680.0      0.0      normr2 = innerp(y.T)  # Norm squared of residual.
   191         1         58.0     58.0      0.0      projections = XTy
   192
   193         1       2166.0   2166.0      0.0      gamma = np.zeros(shape=(n_nonzero_coefs, B), dtype=np.int64)
   194         1     582380.0 582380.0      0.7      F = np.repeat(np.identity(n_nonzero_coefs, dtype=X.dtype)[np.newaxis], B, 0)
   195         1       1335.0   1335.0      0.0      a_F = np.zeros_like(X, shape=(n_nonzero_coefs, B, 1))
   196         1        698.0    698.0      0.0      D_mybest = np.empty_like(X, shape=(n_nonzero_coefs, B, XTX.shape[0]))  # empty_like is faster to init
   197         1         17.0     17.0      0.0      temp_F_k_k = 1
   198         1        250.0    250.0      0.0      xests = np.zeros((B, X.shape[1]))
   199       129       3591.0     27.8      0.0      for k in range(n_nonzero_coefs):
   200       128    1381715.0  10794.6      1.7          maxindices = get_max_projections_blas(projections)  # Numba version is about twice as fast as: np.argmax(projections * projections, 1)  # Maybe replace square with abs?
   201       128      17753.0    138.7      0.0          gamma[k] = maxindices
   202       128       2862.0     22.4      0.0          if k == 0:
   203         1      92769.0  92769.0      0.1              D_mybest[k] = XTX[None, maxindices, :]
   204                                                   else:
   205                                                       # Do something about this:
   206       127    2949174.0  23221.8      3.5              D_mybest_maxindices = np.take_along_axis(D_mybest[:k, :, :].transpose([2, 1, 0]), maxindices[None, :, None], 0).squeeze(0)
   207       127     170585.0   1343.2      0.2              temp_F_k_k = np.sqrt(1/(1-innerp(D_mybest_maxindices)))[:, None]
   208       127    7408694.0  58336.2      8.9              F[:, :, k] = -temp_F_k_k * (F[:, :, :k] @ D_mybest_maxindices[:, :, None]).squeeze(-1)
   209       127      41196.0    324.4      0.0              F[:, k, k] = temp_F_k_k[..., 0]
   210                                                       # Number of flops below: (2*k - 1) * D_mybest.shape[1] * D_mybest.shape[2]
   211                                                       # t1 = time.time()
   212       127   56672196.0 446237.8     68.1              D_mybest[k] = (D_mybest[:k].transpose([1, 2, 0]) @ D_mybest_maxindices[:, :, None]).squeeze(-1)  # <- faster than np.einsum('ibj,ib->bj', D_mybest[:k], D_mybest_maxindices)
   213                                                       # t2 = time.time()
   214                                                       # print(((2*k - 1) * D_mybest.shape[1] * D_mybest.shape[2] * 1e-9)/(t2-t1), 'GFLOPS')
   215       127   11482818.0  90415.9     13.8              update_D_mybest(temp_F_k_k, XTX, maxindices, D_mybest[k])
   216       128     245822.0   1920.5      0.3          a_F[k] = temp_F_k_k * np.take_along_axis(projections, maxindices[:, None], 1)
   217       128    1989279.0  15541.2      2.4          update_projections_blast(projections, D_mybest[k], -a_F[k, :, 0])  # Around a 3x speedup :D
   218                                                   # projections2 = projections + (-a_F[k]) * D_mybest[k]  # Relativeely slow as all the subsets are different...
   219       128      38835.0    303.4      0.0          normr2 = normr2 - (a_F[k] * a_F[k]).squeeze(-1)
   220                                               else:
   221         1      87255.0  87255.0      0.1          np.put_along_axis(xests, gamma.T, (F @ a_F.squeeze(-1).T[:, :, None]).squeeze(-1), -1)
   222                                                   # Instead of putting a stopping criteria, we could return the order in which the coeffs were added. (i.e. just return gamma and amplitudes)
   223         1         29.0     29.0      0.0      return xests

Total time: 12.4447 s
File: test_omp.py
Function: omp_naive at line 259

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   259                                           @profile
   260                                           def omp_naive(X, y, n_nonzero_coefs):
   261         1      45751.0  45751.0      0.0      Xt = np.ascontiguousarray(X.T)
   262         1       7228.0   7228.0      0.0      y = np.ascontiguousarray(y.T)
   263         1       4373.0   4373.0      0.0      r = y.copy()  # Maybe no transpose? Remove this line?
   264         1        346.0    346.0      0.0      sets = np.zeros((n_nonzero_coefs, r.shape[0]), dtype=np.int32)
   265         1        209.0    209.0      0.0      problems = np.zeros((r.shape[0], n_nonzero_coefs, X.shape[0]))
   266         1     157982.0 157982.0      0.1      As = np.repeat(np.identity(n_nonzero_coefs, dtype=X.dtype)[np.newaxis], r.shape[0], 0)
   267                                               # solutions = np.zeros((r.shape[0], n_nonzero_coefs))
   268         1      17129.0  17129.0      0.0      xests = np.zeros_like(y, shape=(r.shape[0], X.shape[1]))
   269       129       9614.0     74.5      0.0      for k in range(n_nonzero_coefs):
   270                                                   #t1 = time.time()
   271       128   27389253.0 213978.5     22.0          projections = faster_projections(Xt, r).squeeze(-1)  # X.shape[0] * (2*X.shape[1]-1) * r.shape[0] = O(bNM), where X is an MxN matrix, N>M.
   272                                                   #t2 = time.time()
   273                                                   #print((X.shape[0] * (2*X.shape[1]-1) * r.shape[0] * 1e-9)/(t2-t1), 'GFLOPS')
   274       128   13418196.0 104829.7     10.8          best_idxs = get_max_projections_blas(projections)  # best_idxs = np.abs(projections).squeeze(-1).argmax(1)  # O(bN), https://scikit-cuda.readthedocs.io/en/latest/generated/skcuda.misc.maxabs.html
   275       128      24072.0    188.1      0.0          sets[k, :] = best_idxs
   276       128    1341499.0  10480.5      1.1          best = Xt[best_idxs, :]  # A mess...
   277       128    1257742.0   9826.1      1.0          problems[:, k, :] = best
   278       128      12472.0     97.4      0.0          current_problemst = problems[:, :k+1, :]
   279       128      10982.0     85.8      0.0          current_problems = current_problemst.transpose([0, 2, 1])
   280                                                   # TODO: We have already computed the result of current_problemst @ y[:, :, None]. (It is in projections I believe)
   281                                                   #       And similarly for the hermitian - it can be constructed from XTX.
   282                                                   # LAPACK has dgesvx/dgelsy (seems gelsy is newer/better) - gesv is for solving, these other ones work for least squares!
   283                                                   #  I think linalg.solve calls gesv, which uses LU factorization. https://stackoverflow.com/questions/55367024/fastest-way-of-solving-linear-least-squares
   284       128   10115934.0  79030.7      8.1          update = (current_problemst[:, :k, :] @ best[..., None]).squeeze(-1)
   285                                                   # Our algorithm could be so much faster if the lhs were not (likely) all different.
   286                                                   #  like in batch_mm, we could then treat the rhs as a single matrix.
   287       128     132139.0   1032.3      0.1          As[:, k, :k] = update  # We only have to update the lower triangle for sposv to work!
   288                                                   if True:
   289       128     601013.0   4695.4      0.5              As[:, :k, k] = update
   290       128      11158.0     87.2      0.0              solutions = np.linalg.solve(
   291       128       7230.0     56.5      0.0                   As[:, :k+1, :k+1],  # As[:, :k+1, :k+1],
   292       128   56029958.0 437734.0     45.0                   current_problemst @ y[:, :, None]).squeeze(-1)  # O(bk^2) memory.
   293                                                   else:
   294                                                       # ^ This is faster for small matrices (Python overhead most likely, but may also just be complexity)
   295                                                       solutions = solve_lstsq(current_problems, As[:, :k+1, :k+1], y)
   296       128   13834334.0 108080.7     11.1          r = y - (current_problems @ solutions[:, :, None]).squeeze(-1)
   297
   298                                                   # maybe memoize in case y is large, such that probability of repeats is significant.
   299                                                   # If we can move all the batched stuff to RHS (using dense representations) it may be faster.
   300                                                   # maybe memoize in case y is large, such that probability of repeats is significant. <- In case sparsity is low (e.g. < 32), this may be the fastest method, only limited by memory. (binomial(n, n*0.2
))   else:
   301                                                   # https://en.wikipedia.org/wiki/Singular_value_decomposition#Relation_to_eigenvalue_decomposition
   302                                                   # https://en.wikipedia.org/wiki/Woodbury_matrix_identity
   303                                                   # Test if _^2 is faster than abs(_)
   304                                               else:
   305                                                   # np.put_along_axis(xests.T, sets.T, solutions, 1)
   306         1      18229.0  18229.0      0.0          np.put_along_axis(xests, sets.T, solutions, 1)
   307
   308         1         42.0     42.0      0.0      return xests



LATER
Settings used for the test:
Number of Samples: 100
Number of Components: 8192
Number of Features: 1024
Number of Nonzero Coefficients: 512


Single core. New implementation of algorithm v0 (blas)
Samples per second: 1.567001694710766


Wrote profile results to test_omp.py.lprof
Timer unit: 1e-07 s

Total time: 60.5411 s
File: ../test_omp.py
Function: omp_v0_new_blas at line 223

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   223                                           @profile
   224                                           def omp_v0_new_blas(y, X, XTX, XTy, n_nonzero_coefs=None):
   225                                               # TODO: Seems to be as fast as we are going to get, without custom kernels. We just need single-block memory alloc. (and send out-argument with pre-allocated me
m to functions)
   226         1         54.0     54.0      0.0      if n_nonzero_coefs is None:
   227                                                   n_nonzero_coefs = X.shape[1]
   228
   229         1         53.0     53.0      0.0      B = y.shape[1]
   230         1         30.0     30.0      0.0      innerp = lambda x: np.einsum('ij,ij->i', x, x)
   231         1       2594.0   2594.0      0.0      normr2 = innerp(y.T)  # Norm squared of residual.
   232         1         32.0     32.0      0.0      projections = XTy
   233
   234         1       4333.0   4333.0      0.0      gamma = np.zeros(shape=(n_nonzero_coefs, B), dtype=np.int64)
   235         1     819768.0 819768.0      0.1      F = np.repeat(np.identity(n_nonzero_coefs, dtype=X.dtype)[np.newaxis], B, 0)
   236         1       4497.0   4497.0      0.0      a_F = np.zeros_like(X, shape=(n_nonzero_coefs, B, 1))
   237         1        850.0    850.0      0.0      D_mybest = np.empty_like(X, shape=(n_nonzero_coefs, B, XTX.shape[0]))  # FIXME: Use empty_like! #empty_like is faster to init
   238         1         44.0     44.0      0.0      temp_F_k_k = 1
   239         1        273.0    273.0      0.0      xests = np.zeros((B, X.shape[1]))
   240       513      16144.0     31.5      0.0      for k in range(n_nonzero_coefs):
   241       512    3838318.0   7496.7      0.6          maxindices = get_max_projections_blas(projections)  # Numba version is about twice as fast as: np.argmax(projections * projections, 1)  # Maybe replace squa
re with abs?
   242       512      75087.0    146.7      0.0          gamma[k] = maxindices
   243       512      12160.0     23.8      0.0          if k == 0:
   244         1      50653.0  50653.0      0.0              D_mybest[k] = XTX[None, maxindices, :]
   245                                                   else:
   246                                                       # Do something about this:
   247       511   12300945.0  24072.3      2.0              D_mybest_maxindices = np.take_along_axis(D_mybest[:k, :, :].transpose([2, 1, 0]), maxindices[None, :, None], 0).squeeze(0)
   248       511     598748.0   1171.7      0.1              temp_F_k_k = np.sqrt(1/(1-innerp(D_mybest_maxindices)))[:, None]
   249       511   52441717.0 102625.7      8.7              F[:, :, k] = -temp_F_k_k * (F[:, :, :k] @ D_mybest_maxindices[:, :, None]).squeeze(-1)
   250       511      86022.0    168.3      0.0              F[:, k, k] = temp_F_k_k[..., 0]
   251                                                       # t1 = time.time()
   252       511  526841684.0 1031001.3     87.0              update_D_mybest_blast(temp_F_k_k[:, 0], XTX, maxindices, D_mybest[:k].transpose([1, 2, 0]), D_mybest_maxindices, D_mybest[k])
   253                                                       # t2 = time.time()
   254                                                       # print(D_mybest.shape[1] * ((2*k - 1) * D_mybest.shape[2] + 2 * D_mybest.shape[2])/(t2-t1) * 1e-9, 'GFLOPS')
   255
   256       512     827202.0   1615.6      0.1          a_F[k] = temp_F_k_k * np.take_along_axis(projections, maxindices[:, None], 1)
   257       512    7013571.0  13698.4      1.2          update_projections_blast(projections, D_mybest[k], -a_F[k, :, 0])  # Around a 3x speedup :D
   258                                                   # projections2 = projections + (-a_F[k]) * D_mybest[k]  # Relativeely slow as all the subsets are different...
   259       512     147182.0    287.5      0.0          normr2 = normr2 - (a_F[k] * a_F[k]).squeeze(-1)
   260                                               else:
   261         1     329341.0 329341.0      0.1          np.put_along_axis(xests, gamma.T, (F @ a_F.squeeze(-1).T[:, :, None]).squeeze(-1), -1)
   262                                                   # Instead of putting a stopping criteria, we could return the order in which the coeffs were added. (i.e. just return gamma and amplitudes)
   263         1         73.0     73.0      0.0      return xests
